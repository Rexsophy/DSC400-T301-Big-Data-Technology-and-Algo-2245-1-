{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7befb43b-e573-43ab-858f-976720a9e847",
      "metadata": {
        "id": "7befb43b-e573-43ab-858f-976720a9e847"
      },
      "source": [
        "#### Rex Gayas\n",
        "#### Week 9 Exercise 9.2 Spring 2024\n",
        "#### DSC400-T301 Big Data, Technology, and Algo (2245-1)\n",
        "#### Clustering with Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 9"
      ],
      "metadata": {
        "id": "plnvp7sIzRwh"
      },
      "id": "plnvp7sIzRwh"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark Session for this assignment\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"DSC 400 Assignment 9\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Paths to the datasets\n",
        "sample_kmeans_data_path = \"data/mllib/sample_kmeans_data.txt\"\n",
        "sample_lda_data_path = \"data/mllib/sample_lda_data_path.txt\"\n",
        "sample_movielens_data_path = \"data/mllib/sample_movielens_data_path.txt\"\n"
      ],
      "metadata": {
        "id": "57Z7yarLz8x1"
      },
      "id": "57Z7yarLz8x1",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 9.1"
      ],
      "metadata": {
        "id": "906rBP5Y0MRA"
      },
      "id": "906rBP5Y0MRA"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "import requests\n",
        "\n",
        "# Function to download the file\n",
        "def download_data(url, local_filename):\n",
        "    response = requests.get(url)\n",
        "    with open(local_filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "# Download k-means data\n",
        "data_url = \"https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_kmeans_data.txt\"\n",
        "local_data_path = \"sample_kmeans_data.txt\"\n",
        "download_data(data_url, local_data_path)\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"KMeans Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load and parse data\n",
        "data = spark.read.format(\"libsvm\").load(local_data_path)\n",
        "\n",
        "# Trains a k-means model\n",
        "kmeans = KMeans().setK(2).setSeed(1)\n",
        "model = kmeans.fit(data)\n",
        "\n",
        "# Evaluate clustering by computing the Silhouette score\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(model.transform(data))\n",
        "print(f\"Silhouette with squared euclidean distance = {silhouette}\")\n",
        "\n",
        "# Shows the result\n",
        "centers = model.clusterCenters()\n",
        "print(\"Cluster Centers: \")\n",
        "for center in centers:\n",
        "    print(center)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgkiAsuv1vmJ",
        "outputId": "f136bb76-4452-4a93-dcb7-7334ea14801e"
      },
      "id": "sgkiAsuv1vmJ",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette with squared euclidean distance = 0.9997530305375207\n",
            "Cluster Centers: \n",
            "[9.1 9.1 9.1]\n",
            "[0.1 0.1 0.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilized Apache Spark’s MLlib to perform k-means clustering on a dataset provided via a URL. The dataset was first downloaded using the Python “requests” library, and then loaded into a Spark DataFrame formatted as \"libsvm\". Then set up a k-means clustering model specifying 2 clusters and a random initialization mode. After training the model with the data, PySpark's “ClusteringEvaluator” computed the silhouette score, which assessed the clustering quality.\n",
        "The output indicates a silhouette score of approximately 0.9997, suggesting that the clusters are very well-defined and separate from each other. The cluster centers are listed as [9.1, 9.1, 9.1] and [0.1, 0.1, 0.1], reflecting the centroids of the two clusters determined by the algorithm. These centers represent the average position of all points within each cluster, illustrating the typical characteristics of the grouped data points.\n"
      ],
      "metadata": {
        "id": "cfZTKHDb2YN5"
      },
      "id": "cfZTKHDb2YN5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 9.2"
      ],
      "metadata": {
        "id": "Izc5h2Vb2bns"
      },
      "id": "Izc5h2Vb2bns"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import LDA\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "# Function to download the file\n",
        "def download_data(url, local_filename):\n",
        "    response = requests.get(url)\n",
        "    with open(local_filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "# Download LDA data\n",
        "data_url = \"https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_lda_libsvm_data.txt\"\n",
        "local_data_path = \"sample_lda_libsvm_data.txt\"\n",
        "download_data(data_url, local_data_path)\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"LDA Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load and parse data\n",
        "data = spark.read.format(\"libsvm\").load(local_data_path)\n",
        "\n",
        "# Trains a LDA model\n",
        "lda = LDA(k=3, maxIter=10)\n",
        "model = lda.fit(data)\n",
        "\n",
        "# Shows the result\n",
        "transformed = model.transform(data)\n",
        "topics = model.describeTopics(maxTermsPerTopic=3)\n",
        "print(\"Topics described by their top-weighted terms:\")\n",
        "topics.show(truncate=False)\n",
        "\n",
        "# Shows the transformed (topic distributions for documents)\n",
        "print(\"Transformed document (topic distributions):\")\n",
        "transformed.show(truncate=False)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AVwbk-_3JKT",
        "outputId": "755a13b9-cae0-43af-b01c-987b4b67e6be"
      },
      "id": "-AVwbk-_3JKT",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics described by their top-weighted terms:\n",
            "+-----+-----------+---------------------------------------------------------------+\n",
            "|topic|termIndices|termWeights                                                    |\n",
            "+-----+-----------+---------------------------------------------------------------+\n",
            "|0    |[3, 8, 2]  |[0.09914672539329934, 0.09517768603815174, 0.09401299772489345]|\n",
            "|1    |[6, 10, 9] |[0.22164169772396472, 0.17481235649818788, 0.10521607779173928]|\n",
            "|2    |[4, 0, 5]  |[0.1550551021060799, 0.14768849157620403, 0.14561601537206648] |\n",
            "+-----+-----------+---------------------------------------------------------------+\n",
            "\n",
            "Transformed document (topic distributions):\n",
            "+-----+---------------------------------------------------------------+--------------------------------------------------------------+\n",
            "|label|features                                                       |topicDistribution                                             |\n",
            "+-----+---------------------------------------------------------------+--------------------------------------------------------------+\n",
            "|0.0  |(11,[0,1,2,4,5,6,7,10],[1.0,2.0,6.0,2.0,3.0,1.0,1.0,3.0])      |[0.26992518347009326,0.2375312352170176,0.4925435813128891]   |\n",
            "|1.0  |(11,[0,1,3,4,7,10],[1.0,3.0,1.0,3.0,2.0,1.0])                  |[0.025153264363384078,0.030215442435901844,0.944631293200714] |\n",
            "|2.0  |(11,[0,1,2,5,6,8,9],[1.0,4.0,1.0,4.0,9.0,1.0,2.0])             |[0.013176189421876452,0.6118339404523536,0.37498987012576984] |\n",
            "|3.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,3.0,9.0])            |[0.011453915443881166,0.9734089390640732,0.015137145492045657]|\n",
            "|4.0  |(11,[0,1,2,3,4,6,9,10],[3.0,1.0,1.0,9.0,3.0,2.0,1.0,3.0])      |[0.012781854218610198,0.400880322396428,0.5863378233849619]   |\n",
            "|5.0  |(11,[0,1,3,4,5,6,7,8,9],[4.0,2.0,3.0,4.0,5.0,1.0,1.0,1.0,4.0]) |[0.011617389482665457,0.01314175415107339,0.9752408563662612] |\n",
            "|6.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,2.0,9.0])            |[0.01190691700655413,0.9723382836366958,0.015754799356749977] |\n",
            "|7.0  |(11,[0,1,2,3,4,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,1.0,2.0,1.0,3.0])|[0.013994146164727123,0.4767063867965173,0.5092994670387556]  |\n",
            "|8.0  |(11,[0,1,3,4,5,6,7],[4.0,4.0,3.0,4.0,2.0,1.0,3.0])             |[0.013457013983759826,0.01473854043777031,0.9718044455784699] |\n",
            "|9.0  |(11,[0,1,2,4,6,8,9,10],[2.0,8.0,2.0,3.0,2.0,2.0,7.0,2.0])      |[0.010696666406145913,0.466991654243362,0.5223116793504922]   |\n",
            "|10.0 |(11,[0,1,2,3,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,2.0,3.0,3.0])      |[0.013404785478155051,0.6523423941171973,0.33425282040464754] |\n",
            "|11.0 |(11,[0,1,4,5,6,7,9],[4.0,1.0,4.0,5.0,1.0,3.0,1.0])             |[0.014777318902383053,0.016183702035392104,0.9690389790622248]|\n",
            "+-----+---------------------------------------------------------------+--------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Latent Dirichlet Allocation (LDA) model was applied to a dataset using PySpark, identifying the major topics present within the documents. Each topic is described by key terms with associated weights that signify their relevance. The output also shows each document's distribution across these topics, indicating the percentage contribution of each topic to individual documents. This helps in understanding the thematic structure of the text data, making it easier to categorize and summarize the documents based on their content."
      ],
      "metadata": {
        "id": "MsI5kMza3_Hh"
      },
      "id": "MsI5kMza3_Hh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 9.3"
      ],
      "metadata": {
        "id": "nWNhKvQT4Brv"
      },
      "id": "nWNhKvQT4Brv"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Function to download the file\n",
        "def download_data(url, local_filename):\n",
        "    response = requests.get(url)\n",
        "    with open(local_filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "# Download the dataset\n",
        "data_url = \"https://raw.githubusercontent.com/apache/spark/master/data/mllib/als/sample_movielens_ratings.txt\"\n",
        "local_data_path = \"sample_movielens_ratings.txt\"\n",
        "download_data(data_url, local_data_path)\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Collaborative Filtering Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load and parse data into DataFrame\n",
        "lines = spark.read.text(local_data_path).rdd\n",
        "parts = lines.map(lambda row: row.value.split(\"::\"))\n",
        "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
        "                                     rating=float(p[2]), timestamp=int(p[3])))\n",
        "ratings = spark.createDataFrame(ratingsRDD)\n",
        "\n",
        "# Split data into training and test sets\n",
        "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Build the recommendation model using ALS on the training data\n",
        "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
        "          coldStartStrategy=\"drop\", implicitPrefs=False)\n",
        "model = als.fit(training)\n",
        "\n",
        "# Evaluate the model by computing the RMSE on the test data\n",
        "predictions = model.transform(test)\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
        "                                predictionCol=\"prediction\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root-mean-square error = \" + str(rmse))\n",
        "\n",
        "# Generate top 10 movie recommendations for each user\n",
        "userRecs = model.recommendForAllUsers(10)\n",
        "userRecs.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdY-jUsC4ECI",
        "outputId": "3f6e51c5-cff9-4808-888a-8bc726573374"
      },
      "id": "fdY-jUsC4ECI",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root-mean-square error = 1.6518418668707993\n",
            "+------+--------------------+\n",
            "|userId|     recommendations|\n",
            "+------+--------------------+\n",
            "|    20|[{22, 4.673408}, ...|\n",
            "|    10|[{46, 3.9353182},...|\n",
            "|     0|[{72, 4.232644}, ...|\n",
            "|     1|[{62, 3.9370146},...|\n",
            "|    21|[{29, 4.984131}, ...|\n",
            "|    11|[{18, 5.2808976},...|\n",
            "|    12|[{28, 5.610158}, ...|\n",
            "|    22|[{88, 5.0526786},...|\n",
            "|     2|[{7, 5.622658}, {...|\n",
            "|    13|[{93, 3.916977}, ...|\n",
            "|     3|[{51, 5.0403833},...|\n",
            "|    23|[{46, 5.456856}, ...|\n",
            "|     4|[{93, 3.9825988},...|\n",
            "|    24|[{90, 5.155915}, ...|\n",
            "|    14|[{52, 5.0879784},...|\n",
            "|     5|[{55, 4.4657507},...|\n",
            "|    15|[{46, 4.8242826},...|\n",
            "|    25|[{96, 4.108087}, ...|\n",
            "|    26|[{75, 6.6279454},...|\n",
            "|     6|[{25, 5.0263276},...|\n",
            "+------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemented a collaborative filtering model using the Alternating Least Squares (ALS) algorithm in PySpark. The model was trained on a dataset from the Movielens project, which consists of user-movie ratings. After splitting this data into training and test sets, the ALS algorithm was used to predict movie ratings for the test set, and the model's performance was evaluated using the Root Mean Square Error (RMSE) metric. The output shows an RMSE of approximately 1.588, which quantifies the average prediction error in terms of the ratings' scale. Lower RMSE values generally indicate better model performance, suggesting that the model predictions are relatively close to the actual ratings. Also listed are the top 10 movie recommendations for each user, generated by the model. These recommendations are tailored for individual users based on their ratings history, showcasing the model's ability to leverage user and item interactions to personalize content."
      ],
      "metadata": {
        "id": "Pyyg6JdN6GJQ"
      },
      "id": "Pyyg6JdN6GJQ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}